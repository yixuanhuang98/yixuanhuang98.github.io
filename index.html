<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yixuan Huang</title>

    <meta name="author" content="Yixuan Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <style>
      body {
        background-color: #f9f9f9;
      }
    </style>

    <style>
      .accordion {
        background-color: #eee;
        color: #444;
        cursor: pointer;
        padding: 18px;
        width: 100%;
        border: none;
        text-align: left;
        outline: none;
        font-size: 22px;
        font-family: monospace;
        transition: 0.4s;
      }

      .active, .accordion:hover {
        background-color: #ccc;
      }

      .panel {
        padding: 0 0px;
        width:100%;
        vertical-align:middle;
        background-color: #f9f9f9;
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.4s ease-out;
      }
      
      /* Style the button that is used to open and close the collapsible content */
      .collapsible {
        background-color: #f9f9f9;
        /* color: #444; */
        cursor: pointer;
        /* padding: 18px; */
        width: 100%;
        border: none;
        text-align: right;
        outline: none;
        /* font-size: 15px; */
        font-family: monospace;
        transition: 0.4s;
      }

      /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
      .active, .collapsible:hover {
        background-color: #ccc;
      }

      /* Style the collapsible content. Note: hidden by default */
      .content {
        /* padding: 0 18px; */
        display: none;
        overflow: hidden;
        background-color: #f9f9f9;
        transition: max-height 0.4s ease-out;
      }
    </style>  
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yixuan Huang
                </p>
                <p>
                  Hello, my name is Yixuan Huang. I'm a final-year Ph.D. candidate in the Kahlert School of Computing at the University of Utah. 
                  I work on algorithms to reason about inter-object relations and how these relations change based on long-horizon robot actions in diverse household environments. 
                </p>
                <p>
                  I am fortunate to be advised by <a href="https://robot-learning.cs.utah.edu/thermans">Prof. Tucker Hermans</a>. 
                  I was a Visiting Student Researcher (VSR) at Stanford University, where I worked with <a href="https://web.stanford.edu/~bohg/">Prof. Jeannette Bohg</a> in the <a href="https://iprl.stanford.edu/">Interactive Perception and Robot Learning Lab</a>.
                  I got my bachelor's degree in Computer Science and Engineering from Northeastern University in 2020. During my undergraduate years, I worked with <a href="https://scungao.github.io/">Prof. Sicun Gao</a> at UC San Diego. 
                </p>
                <p>
                  <strong>I am on the job market for postdoc and industry research scientist positions. Please reach out if you are interested in my research!</strong>
                </p>
                <p style="text-align:center">
                  yixuan.huang[at]utah.edu &nbsp/&nbsp; 
                  <a href="data/Yixuan_Huang_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=m0XFTIoAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yixuanhuang98">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yixuan-huang-38b73617b/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yixuan_2023.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yixuan_2022.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
      <tr>
        <td style="text-align: left; background-color: #eee">
          <heading style="font-family: monospace; font-size: 22px; color: #444">Research</heading>
        </td>
      </tr>
      </tbody>
    </table>

    <ul>
    My research goal is to ground logical goals and sensory information in robot skills. To achieve this, I study planning for robot manipulation with learned relational dynamics. Specifically, I focus on learning to predict <strong>relational dynamics</strong> from <strong>partial-view point clouds</strong>, which enables the robot to solve challenging real-world tasks including:
    </ul>
    <ul style="margin-top: 20px; margin-bottom: 20px;">
      <li style="margin-bottom: 5px;"> Planning to logical goals with manipulating <strong>multiple dynamically interacting objects</strong> (<a href="https://sites.google.com/view/rdgnn" target="_blank">RD-GNN</a>). </li>
      <li style="margin-bottom: 5px;"> Reasoning about how the manipulated objects interact with the <strong>structural elements of the environment</strong>  (<a href="https://sites.google.com/view/erelationaldynamics" target="_blank">eRDTransformer</a>). </li>
      <li style="margin-bottom: 5px;"> Leveraging objects-oriented memory with a video tracker to reason about <strong>occluded objects</strong>  (<a href="https://sites.google.com/view/rdmemory" target="_blank">DOOM and LOOM</a>). </li>
      <li style="margin-bottom: 5px;"> Solving <strong>long-horizon manipulation tasks</strong> with a composable planning framework  (<a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a>). </li>
      <li style="margin-bottom: 35px;"> Detecting failure cases, recovering from them, and improving the dynamics model using these cases (Ongoing). </li>
    </ul>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
      <tr>
        <td style="text-align: left; background-color: #eee">
          <heading style="font-family: monospace; font-size: 22px; color: #444">Featured Publications</heading>
        </td>
      </tr>
      </tbody>
    </table>


    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="p2p_stop()" onmouseover="p2p_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/p2p_packing.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/p2p_apple.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2408.14769">
            <span class="papertitle">Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://www.chrisagia.com/">Chirstopher Agia</a>, <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>.
          <br>
          Under Review
          <br>
          <a href="https://sites.google.com/stanford.edu/points2plans">project page</a> / 
          <a href="https://arxiv.org/pdf/2408.14769">arXiv</a> /
          <a href="https://github.com/yixuanhuang98/Points2Plans">code</a>
          <p></p>
        </td>
      </tr>

      <tr onmouseout="memory_stop()" onmouseover="memory_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/memory_task_1.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/memory_task_2.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.15278.pdf">
            <span class="papertitle">Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://jia2lin3yuan1.github.io/">Jialin Yuan</a>, <a href="https://scholar.google.com/citations?user=xARSfT4AAAAJ&hl=en">Chanho Kim</a>, <a href="https://pupulpradhan9.wixsite.com/home">Pupul Pradhan</a>, Bryan Chen, <a href="https://web.engr.oregonstate.edu/~lif/">Li Fuxin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://2024.ieee-icra.org/index.html">International Conference on Robotics and Automation</a> (ICRA) 2024
          <br>
          <a href="https://sites.google.com/view/rdmemory">project page</a> / 
          <a href="https://arxiv.org/pdf/2309.15278.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>


      <tr onmouseout="eRD_stop()" onmouseover="eRD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/erd_all.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2305.10857.pdf">
            <span class="papertitle">Latent Space Planning for Multi-Object Manipulation with Environment-Aware Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://nicholscrawford.github.io/">Nichols Crawford Taylor</a>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="http://weiyuliu.com/">Weiyu Liu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.ieee-ras.org/publications/t-ro">IEEE Transactions on Robotics</a> (T-RO) 2024
          <br>
          <a href="https://sites.google.com/view/erelationaldynamics">project page</a> / 
          <a href="https://arxiv.org/pdf/2305.10857.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/erdtransformer/src/main/">code</a>
          <p></p>
        </td>
      </tr>
      
      <tr onmouseout="RD_stop()" onmouseover="RD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/rd_multi_step.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2209.11943.pdf">
            <span class="papertitle">Planning for Multi-Object Manipulation with Graph Neural Network Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.icra2023.org/programme/call-contributions">International Conference on Robotics and Automation</a> (ICRA) 2023 
          <br>
          <a href="https://sites.google.com/view/rdgnn">project page</a> / 
          <a href="https://arxiv.org/pdf/2209.11943.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/rdgnn_icra_2023/src/master/">code</a>
          <p></p>
        </td>
      </tr>

      <tr onmouseout="tendon_stop()" onmouseover="tendon_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/tendon_video.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2110.07789.pdf">
            <span class="papertitle">Toward Learning Context-Dependent Tasks from Demonstration for Tendon-Driven Surgical Robots</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://scholar.google.com/citations?user=MbeZEoMAAAAJ&hl=en">Michael Bentley</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a>.
          <br>
          <a href="http://www.ismr.gatech.edu/">International Symposium on Medical Robotics</a> (ISMR) 2021
          <br>
          <strong>Best Paper Award Finalist, Best Student Paper Award Finalist</strong>
          <br>
          <a href="https://robot-learning.cs.utah.edu/project:lfd_tendon_robot">project page</a> / 
          <a href="https://arxiv.org/pdf/2110.07789.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>

    </table>

    <button class="accordion">All Publications</button>
    <div class="panel">
    <p></p>

    

    
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="p2p_stop()" onmouseover="p2p_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/p2p_packing.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/p2p_apple.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2408.14769">
            <span class="papertitle">Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://www.chrisagia.com/">Chirstopher Agia</a>, <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>.
          <br>
          Under Review
          <br>
          <a href="https://sites.google.com/stanford.edu/points2plans">project page</a> / 
          <a href="https://arxiv.org/pdf/2408.14769">arXiv</a> /
          <a href="https://github.com/yixuanhuang98/Points2Plans">code</a>
          <p></p>
        </td>
      </tr>
    </table>
    
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="memory_stop()" onmouseover="memory_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/memory_task_1.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/memory_task_2.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.15278.pdf">
            <span class="papertitle">Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://jia2lin3yuan1.github.io/">Jialin Yuan</a>, <a href="https://scholar.google.com/citations?user=xARSfT4AAAAJ&hl=en">Chanho Kim</a>, <a href="https://pupulpradhan9.wixsite.com/home">Pupul Pradhan</a>, Bryan Chen, <a href="https://web.engr.oregonstate.edu/~lif/">Li Fuxin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://2024.ieee-icra.org/index.html">International Conference on Robotics and Automation</a> (ICRA) 2024
          <br>
          <a href="https://sites.google.com/view/rdmemory">project page</a> / 
          <a href="https://arxiv.org/pdf/2309.15278.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>


      <tr onmouseout="eRD_stop()" onmouseover="eRD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/erd_all.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2305.10857.pdf">
            <span class="papertitle">Latent Space Planning for Multi-Object Manipulation with Environment-Aware Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://nicholscrawford.github.io/">Nichols Crawford Taylor</a>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="http://weiyuliu.com/">Weiyu Liu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.ieee-ras.org/publications/t-ro">IEEE Transactions on Robotics</a> (T-RO) 2024
          <br>
          <a href="https://sites.google.com/view/erelationaldynamics">project page</a> / 
          <a href="https://arxiv.org/pdf/2305.10857.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/erdtransformer/src/main/">code</a>
          <p></p>
        </td>
      </tr>
      
      <tr onmouseout="memoryw_stop()" onmouseover="memoryw_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/memory_apple.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/memory_shelf.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/CoRL_NEURL.pdf">
            <span class="papertitle">Reasoning and Planning about Unobserved Objects with Memory Models</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>
          <br>
          <a href="https://neurl-rmw.github.io/">NeuRL-RMW: Workshop for Neural Representation Learning for Robot Manipulation</a>, CoRL 2023
          <p></p>
        </td>
      </tr>

      <tr onmouseout="eRDw_stop()" onmouseover="eRDw_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/double_pp.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/IROS_casuality.pdf">
            <span class="papertitle">Latent Space Planning for Unobserved Objects with Environment-Aware Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://jia2lin3yuan1.github.io/">Jialin Yuan</a>, <a href="http://weiyuliu.com/">Weiyu Liu</a>, <a href="https://scholar.google.com/citations?user=xARSfT4AAAAJ&hl=en">Chanho Kim</a>, <a href="https://web.engr.oregonstate.edu/~lif/">Li Fuxin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://sites.google.com/view/iros23-causal-robots">Causality for Robotics: Answering the Question of Why </a>, IROS 2023
          <p></p>
        </td>
      </tr>
      
      <tr onmouseout="RD_stop()" onmouseover="RD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/rd_multi_step.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2209.11943.pdf">
            <span class="papertitle">Planning for Multi-Object Manipulation with Graph Neural Network Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.icra2023.org/programme/call-contributions">International Conference on Robotics and Automation</a> (ICRA) 2023 
          <br>
          <a href="https://sites.google.com/view/rdgnn">project page</a> / 
          <a href="https://arxiv.org/pdf/2209.11943.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/rdgnn_icra_2023/src/master/">code</a>
          <p></p>
        </td>
      </tr>

      <tr onmouseout="tendon_stop()" onmouseover="tendon_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/tendon_video.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2110.07789.pdf">
            <span class="papertitle">Toward Learning Context-Dependent Tasks from Demonstration for Tendon-Driven Surgical Robots</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://scholar.google.com/citations?user=MbeZEoMAAAAJ&hl=en">Michael Bentley</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a>.
          <br>
          <a href="http://www.ismr.gatech.edu/">International Symposium on Medical Robotics</a> (ISMR) 2021
          <br>
          <strong>Best Paper Award Finalist, Best Student Paper Award Finalist</strong>
          <br>
          <a href="https://robot-learning.cs.utah.edu/project:lfd_tendon_robot">project page</a> / 
          <a href="https://arxiv.org/pdf/2110.07789.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>
    </table>


    

    </div>
    <p></p>

    

    
    <button class="accordion">Robot Platforms</button>
    <div class="panel">
    <p></p>
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/racecar1.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td>
          <p>
            Undergraduate research project: This project focused on addressing safe reinforcement learning problems. Our goal was to design RL algorithms that maximize cumulative rewards over time while avoiding collisions. I began with a classic racecar example and created a simulation environment using PyBullet. To achieve our goal, we designed two sub-policies to address the objective separately and employed an additional factored policy to select between the sub-policies. Our final evaluation demonstrated that we achieved near-zero violations with low sample complexity during the testing benchmarks.
          </p>
        </td>
      </tr>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/Drone.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td>
          <p>
            Undergraduate research project: In this project, we used a drone to fly around an object and automatically capture a set number of photos for a 3D reconstruction task. We combined a reinforcement learning algorithm with state estimation to find the optimal drone trajectory for achieving high-quality 3D reconstruction.
          </p>
        </td>
      </tr>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/real_tendon_robot.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td>
          <p>
            Ph.D. first-year rotation project: In this work, we take steps toward developing a system capable of learning to perform context-dependent surgical tasks by learning directly from expert demonstrations. To achieve this, we present and evaluate three approaches for generating context variables from the robot's environment. The environment is represented by partial-view point clouds, with approaches ranging from fully human-specified to fully automated.
          </p>
        </td>
      </tr>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/rd_multi_step.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td>
          <p>
            During the middle of my Ph.D., I worked with the <a href="https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/lbr-iiwa">KUKA iiwa robot</a> equipped with a 3-fingered underactuated hand with built-in TakkTile pressure sensors.
            I designed various manipulation primitives (e.g., grasp, place, dump, push) and executed them on the KUKA iiwa robot.
            I published three papers (<a href="https://sites.google.com/view/rdgnn" target="_blank">RD-GNN</a>, <a href="https://sites.google.com/view/erelationaldynamics" target="_blank">eRDTransformer</a>, and <a href="https://sites.google.com/view/rdmemory" target="_blank">DOOM and LOOM</a>) with this robot.
          </p>
        </td>
      </tr>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/p2p_apple.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td>
          <p>
            During my visit to Stanford University, I worked with a customized holonomic mobile base combined with <a href="https://www.kinovarobotics.com/product/gen3-robots">a kinova arm</a>. This robot is capable of navigating, grasping, placing, pushing, pulling, and even tossing! Using these primitives, the robot can even feed you snacks (e.g., an apple). I completed a project (<a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a>) with this impressive robot.
          </p>
        </td>
      </tr>
      <tr onmouseout="racecar_stop()" onmouseover="racecar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/stretch.jpg" alt="tendon" width="140" style="border-style: none">
        </td>
        <td>
          <p>
            For the final project of my Ph.D., I am working with <a href="https://hello-robot.com/stretch-3-product">a Stretch robot</a>. I enjoy working with this lightweight yet capable robot. I will design multiple primitives for the Stretch and focus on my final project, which involves reasoning about failure cases.
          </p>
        </td>
      </tr>
    </tbody></table>
    </div>
    <p></p>
    

    <button class="accordion">Service</button>
    <div class="panel">
    <p></p>

    <ul>
      <li>Program Committe/Reviewer: 
        <ul class="child-list">  
          <li>
            <a href="https://2024.ieee-icra.org/" target="_blank">International Conference on Robotics and Automation (ICRA)</a> (2023, 2024, 2025)
          </li>
          <li>
            <a href="https://www.corl.org/home" target="_blank">Conference on Robot Learning (CoRL)</a> (2023, 2024)
          </li>
          <li>
            <a href="https://roboticsconference.org/" target="_blank">Robotics: Science and Systems (RSS)</a> (2024)
          </li>
          <li>
            <a href="https://ieee-iros.org/" target="_blank">International Conference on Intelligent Robots and Systems (IROS)</a> (2024)
          </li>
          <li>
            <a href="https://www.ieee-ras.org/publications/ra-l" target="_blank">Robotics and Automation Letters (RA-L)</a> (2024)
          </li>
          <li>
            <a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" target="_blank">International Conference on Learning Representations (ICLR)</a> (2025)
          </li>
          <li>
            <a href="https://cis.ieee.org/publications/ieee-transactions-on-artificial-intelligence" target="_blank">IEEE Transactions on Artificial Intelligence (TAI)</a> (2024)
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19" target="_blank">IEEE Transactions on Instrumentation and Measurement (TIM)</a> (2024)
          </li>
          <li>
            <a href="https://leap-workshop.github.io/" target="_blank">Workshop on Learning Effective Abstractions for Planning (LEAP @ CoRL)</a> (2024)
          </li>
        </ul>
        </li> 
      <li> Fall 2022: Teaching mentorship <a href="https://class-schedule.app.utah.edu/asia/1228/description.html?subj=CS&catno=4300&section=001" target="_blank">CS 4300: Artificial Intelligence</a> University of Utah</li>
      <li> Spring 2022: Teaching mentorship <a href="https://class-schedule.app.utah.edu/asia/1228/description.html?subj=CS&catno=4300&section=001" target="_blank">CS 4300: Artificial Intelligence</a> University of Utah</li>
      <li> Summer 2022: Robotics lab tour co-organizer, <a href="https://l2trec.utah.edu/bridge-program/" target="_blank">University of Utah Bridge Program</a></li>
      <li> Summer 2023: Robotics lab tour co-organizer, <a href="https://l2trec.utah.edu/bridge-program/" target="_blank">University of Utah Bridge Program</a></li>
    </ul>
    </div>
    <p></p>

    <button class="accordion">Awards</button>
    <div class="panel">
    <p></p>

    <ul>
      <li>International Symposium on Medical Robotics Best Paper Award Finalist (2021)</li>
      <li>International Symposium on Medical Robotics Best Student Paper Award Finalist (2021)</li>
      <li>National Scholarship by Ministry of Education of China (2017)</li>
      <li>National Scholarship by Ministry of Education of China (2018)</li>
    </ul>
    </div>
    <p></p>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Website source from <a href="https://www.chrisagia.com/">Chris Agia</a>
          </p>
        </td>
      </tr>
    </tbody></table>

    <script>
      var acc = document.getElementsByClassName("accordion");
      var i;

      for (i = 0; i < acc.length; i++) {
        acc[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var panel = this.nextElementSibling;
          if (panel.style.maxHeight) {
            panel.style.maxHeight = null;
          } else {
            panel.style.maxHeight = panel.scrollHeight + "px";
          } 
        });
      }
    </script>

    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;

      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
          } else {
            content.style.display = "block";
          }
        });
      }
    </script>

  </body>
</html>
