<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yixuan Huang</title>

    <meta name="author" content="Yixuan Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yixuan Huang
                </p>
                <p>
                  Hello, my name is Yixuan Huang. I'm a final-year Ph.D. candidate in the Kahlert School of Computing at the University of Utah. 
                  I work on algorithms to reason about inter-object relations and how these relations change based on long-horizon robot actions in diverse household environments. 
                </p>
                <p>
                  I am fortunate to be advised by <a href="https://robot-learning.cs.utah.edu/thermans">Prof. Tucker Hermans</a>. 
                  I was a Visiting Student Researcher (VSR) at Stanford University, where I worked with <a href="https://web.stanford.edu/~bohg/">Prof. Jeannette Bohg</a> in the <a href="https://iprl.stanford.edu/">Interactive Perception and Robot Learning Lab</a>.
                  I got my bachelor's degree in Computer Science and Engineering from Northeastern University in 2020. During my undergraduate years, I worked with <a href="https://scungao.github.io/">Prof. Sicun Gao</a> at UC San Diego. 
                </p>
                <p style="text-align:center">
                  yixuan.huang[at]utah.edu &nbsp/&nbsp; 
                  <a href="data/Yixuan_Huang_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=m0XFTIoAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yixuanhuang98">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yixuan-huang-38b73617b/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yixuan_2023.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yixuan_2022.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    </table>

    <h2>Research</h2>

    <ul>
    My research goal is to ground logical goals and sensory information in robot skills. To achieve this, I study planning for robot manipulation with learned relational dynamics. Specifically, I focus on learning to predict <strong>relational dynamics</strong> from <strong>partial-view point clouds</strong>, which enables the robot to solve challenging real-world tasks including:
    </ul>
    <ul style="margin-top: 10px; margin-bottom: 10px;">
      <li style="margin-bottom: 5px;"> Planning to logical goals with manipulating <strong>multiple dynamically interacting objects</strong> (<a href="https://sites.google.com/view/rdgnn" target="_blank">RD-GNN</a>). </li>
      <li style="margin-bottom: 5px;"> Reasoning about how the manipulated objects interact with the <strong>structural elements of the environment</strong>  (<a href="https://sites.google.com/view/erelationaldynamics" target="_blank">eRDTransformer</a>). </li>
      <li style="margin-bottom: 5px;"> Leveraging objects-oriented memory with a video tracker to reason about <strong>occluded objects</strong>  (<a href="https://sites.google.com/view/rdmemory" target="_blank">DOOM and LOOM</a>). </li>
      <li style="margin-bottom: 5px;"> Solving <strong>long-horizon manipulation tasks</strong> with a composable planning framework  (<a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a>). </li>
      <li style="margin-bottom: 5px;"> Detecting failure cases, recovering from them, and improving the dynamics model using these cases (Ongoing). </li>
    </ul>

    <h2>Preprints</h2>
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="p2p_stop()" onmouseover="p2p_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/p2p_packing.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/p2p_apple.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2408.14769">
            <span class="papertitle">Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://www.chrisagia.com/">Chirstopher Agia</a>, <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>.
          <br>
          Under Review
          <br>
          <a href="https://sites.google.com/stanford.edu/points2plans">project page</a> / 
          <a href="https://arxiv.org/pdf/2408.14769">arXiv</a> /
          <a href="https://github.com/yixuanhuang98/Points2Plans">code</a>
          <p></p>
        </td>
      </tr>
    </table>
    
    <h2>Peer Reviewed Journal/Conference Papers</h2>
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="memory_stop()" onmouseover="memory_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/memory_task_1.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/memory_task_2.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.15278.pdf">
            <span class="papertitle">Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://jia2lin3yuan1.github.io/">Jialin Yuan</a>, <a href="https://scholar.google.com/citations?user=xARSfT4AAAAJ&hl=en">Chanho Kim</a>, <a href="https://pupulpradhan9.wixsite.com/home">Pupul Pradhan</a>, Bryan Chen, <a href="https://web.engr.oregonstate.edu/~lif/">Li Fuxin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://2024.ieee-icra.org/index.html">International Conference on Robotics and Automation</a> (ICRA) 2024
          <br>
          <a href="https://sites.google.com/view/rdmemory">project page</a> / 
          <a href="https://arxiv.org/pdf/2309.15278.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>


      <tr onmouseout="eRD_stop()" onmouseover="eRD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/erd_all.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2305.10857.pdf">
            <span class="papertitle">Latent Space Planning for Multi-Object Manipulation with Environment-Aware Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://nicholscrawford.github.io/">Nichols Crawford Taylor</a>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="http://weiyuliu.com/">Weiyu Liu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.ieee-ras.org/publications/t-ro">IEEE Transactions on Robotics</a> (T-RO) 2024
          <br>
          <a href="https://sites.google.com/view/erelationaldynamics">project page</a> / 
          <a href="https://arxiv.org/pdf/2305.10857.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/erdtransformer/src/main/">code</a>
          <p></p>
        </td>
      </tr>
      
      <tr onmouseout="RD_stop()" onmouseover="RD_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/rd_multi_step.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2209.11943.pdf">
            <span class="papertitle">Planning for Multi-Object Manipulation with Graph Neural Network Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://adamconkey.github.io/">Adam Conkey</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://www.icra2023.org/programme/call-contributions">International Conference on Robotics and Automation</a> (ICRA) 2023 
          <br>
          <a href="https://sites.google.com/view/rdgnn">project page</a> / 
          <a href="https://arxiv.org/pdf/2209.11943.pdf">arXiv</a> / 
          <a href="https://bitbucket.org/robot-learning/rdgnn_icra_2023/src/master/">code</a>
          <p></p>
        </td>
      </tr>

      <tr onmouseout="tendon_stop()" onmouseover="tendon_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/tendon_video.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2110.07789.pdf">
            <span class="papertitle">Toward Learning Context-Dependent Tasks from Demonstration for Tendon-Driven Surgical Robots</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://scholar.google.com/citations?user=MbeZEoMAAAAJ&hl=en">Michael Bentley</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a>.
          <br>
          <a href="http://www.ismr.gatech.edu/">International Symposium on Medical Robotics</a> (ISMR) 2021
          <br>
          <strong>Best Paper Award Finalist, Best Student Paper Award Finalist</strong>
          <br>
          <a href="https://robot-learning.cs.utah.edu/project:lfd_tendon_robot">project page</a> / 
          <a href="https://arxiv.org/pdf/2110.07789.pdf">arXiv</a>
          <p></p>
        </td>
      </tr>
    </table>

    <h2>Peer Reviewed Workshop Papers</h2>
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr onmouseout="p2p_stop()" onmouseover="p2p_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/p2p_drawer.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/p2p_retrival.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/CoRL_LEAP.pdf">
            <span class="papertitle">Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://www.chrisagia.com/">Chirstopher Agia</a>, <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>.
          <br>
          <a href="https://leap-workshop.github.io/">2nd Workshop on Learning Effective Abstractions for Planning (LEAP)</a>, CoRL 2024 <strong>Oral</strong>
          <p></p>
        </td>
      </tr>

      <tr onmouseout="memoryw_stop()" onmouseover="memoryw_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/memory_apple.mp4" type="video/mp4" muted autoplay loop></video>
          <video width="140" src="images/memory_shelf.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/CoRL_NEURL.pdf">
            <span class="papertitle">Reasoning and Planning about Unobserved Objects with Memory Models</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>
          <br>
          <a href="https://neurl-rmw.github.io/">NeuRL-RMW: Workshop for Neural Representation Learning for Robot Manipulation</a>, CoRL 2023
          <p></p>
        </td>
      </tr>

      <tr onmouseout="eRDw_stop()" onmouseover="eRDw_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video width="140" src="images/double_pp.mp4" type="video/mp4" muted autoplay loop></video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/IROS_casuality.pdf">
            <span class="papertitle">Latent Space Planning for Unobserved Objects with Environment-Aware Relational Classifiers</span>
          </a>
          <br>
          <br>
          <strong>Yixuan Huang</strong>, <a href="https://jia2lin3yuan1.github.io/">Jialin Yuan</a>, <a href="http://weiyuliu.com/">Weiyu Liu</a>, <a href="https://scholar.google.com/citations?user=xARSfT4AAAAJ&hl=en">Chanho Kim</a>, <a href="https://web.engr.oregonstate.edu/~lif/">Li Fuxin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>.
          <br>
          <a href="https://sites.google.com/view/iros23-causal-robots">Causality for Robotics: Answering the Question of Why </a>, IROS 2023
          <p></p>
        </td>
      </tr>
    </table>
    

    <h2>Services</h2>

    <ul>
      <li>Program Committe/Reviewer: 
        <a href="https://2024.ieee-icra.org/" target="_blank">ICRA</a> (2023, 2024, 2025),
        <a href="https://www.corl.org/home" target="_blank">CoRL</a> (2023, 2024),
        <a href="https://roboticsconference.org/" target="_blank">RSS</a> (2024), 
        <a href="https://ieee-iros.org/" target="_blank">IROS</a> (2024),
        <a href="https://www.ieee-ras.org/publications/ra-l" target="_blank">RA-L</a> (2024),
        <a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" target="_blank">ICLR</a> (2025),
        <a href="https://cis.ieee.org/publications/ieee-transactions-on-artificial-intelligence" target="_blank">TAI</a> (2024), 
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19" target="_blank">TIM</a> (2024), and
        <a href="https://leap-workshop.github.io/" target="_blank">CoRL LEAP workshop</a> (2024)
      </li> 
      <li> Fall 2022: Teaching mentorship <a href="https://class-schedule.app.utah.edu/asia/1228/description.html?subj=CS&catno=4300&section=001" target="_blank">CS 4300: Artificial Intelligence</a> University of Utah</li>
      <li> Spring 2022: Teaching mentorship <a href="https://class-schedule.app.utah.edu/asia/1228/description.html?subj=CS&catno=4300&section=001" target="_blank">CS 4300: Artificial Intelligence</a> University of Utah</li>
      <li> Summer 2022: Robotics lab tour co-organizer, <a href="https://l2trec.utah.edu/bridge-program/" target="_blank">University of Utah Bridge Program</a></li>
      <li> Summer 2023: Robotics lab tour co-organizer, <a href="https://l2trec.utah.edu/bridge-program/" target="_blank">University of Utah Bridge Program</a></li>
    </ul>

    <h2>Awards</h2>

    <ul>
      <li>International Symposium on Medical Robotics Best Paper Award Finalist (2021)</li>
      <li>International Symposium on Medical Robotics Best Student Paper Award Finalist (2021)</li>
      <li>National Scholarship by Ministry of Education of China (2017)</li>
      <li>National Scholarship by Ministry of Education of China (2018)</li>
    </ul>

  </body>
</html>
